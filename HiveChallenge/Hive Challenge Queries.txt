




Scenario Based questions:

Will the reducer work or not if you use “Limit 1” in any HiveQL query?
Ans: No, it will not run.

Suppose I have installed Apache Hive on top of my Hadoop cluster using default metastore configuration. Then, what will happen if we have multiple clients trying to access Hive at the same time? 
Ans: Default metastore is Derby which only allows 1 session, so it won't allow multiple connections.

Suppose, I create a table that contains details of all the transactions done by the customers: CREATE TABLE transaction_details (cust_id INT, amount FLOAT, month STRING, country STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’ ;
Now, after inserting 50,000 records in this table, I want to know the total revenue generated for each month. But, Hive is taking too much time in processing this query. How will you solve this problem and list the steps that I will be taking in order to do so?
Ans: Since records are too many, we can partition the table on the basis of month, which will  increase the query performance. For that we can perform the follwing steps:
1. First create another table with same schema as given table but additional clause of partitioned by month.
2. Then insert the records from given table with dynamic partioning (set the reqd. flags prior to that).
3. Then we can use the new table.


How can you add a new partition for the month December in the above partitioned table?
Ans: We can add it with "Alter table tablename add partition (month="December") location /user/hive/warehouse/prac.db/tablename"

I am inserting data into a table based on partitions dynamically. But, I received an error – FAILED ERROR IN SEMANTIC ANALYSIS: Dynamic partition strict mode requires at least one static partition column. How will you remove this error?
Ans: The dynamic partioning flags (hive.exec.dynamic.partioning =true and hive.exec.dynamic.partioning.mode=nonstrict ) need to be set accordingly to avoid this error. 



Suppose, I have a CSV file – ‘sample.csv’ present in ‘/temp’ directory with the following entries:
id first_name last_name email gender ip_address
How will you consume this CSV file into the Hive warehouse using built-in SerDe?
Ans: we can use the basic Serde Library "org.apache.hadoop.hive.serde2.OpenCSVSerde"



Suppose, I have a lot of small CSV files present in the input directory in HDFS and I want to create a single Hive table corresponding to these files. The data in these files are in the format: {id, name, e-mail, country}. Now, as we know, Hadoop performance degrades when we use lots of small files.
So, how will you solve this problem where we want to create a single Hive table for lots of small files without degrading the performance of the system?
Ans: WE can create an external table and point its location to the input directory containing files.


LOAD DATA LOCAL INPATH ‘Home/country/state/’
OVERWRITE INTO TABLE address;

The following statement failed to execute. What can be the cause?
Ans: There might be a permission issue with the warehouse directory of this table, because when overwrite is called it may try to remove the previous of the directory. 

Is it possible to add 100 nodes when we already have 100 nodes in Hive? If yes, how?















Hive Practical questions:

Hive Join operations

Create a  table named CUSTOMERS(ID | NAME | AGE | ADDRESS   | SALARY)
Create a Second  table ORDER(OID | DATE | CUSTOMER_ID | AMOUNT
)


Now perform different joins operations on top of these tables
(Inner JOIN, LEFT OUTER JOIN ,RIGHT OUTER JOIN ,FULL OUTER JOIN)

ANS:
1. create table customers
(
id int,
name string,
age int,
address string,
salary int
)
row format delimited
fields terminated by ','
tblproperties ('skip.header.line.count'='1');

2. create table orders
(
oid int,
dt date,
cust_id int,
amount int
)
row format delimited
fields terminated by ','
tblproperties ('skip.header.line.count'='1');



3. select a.name,b.oid,b.amount,b.dt from customers a
inner join orders b 
on a.id=b.cust_id;

4. select a.name,b.oid,b.amount,b.dt from customers a
left outer join orders b 
on a.id=b.cust_id;

5.  select a.name,b.oid,b.amount,b.dt from customers a
right outer join orders b 
on a.id=b.cust_id;

6.  select a.name,b.oid,b.amount,b.dt from customers a
full outer join orders b;


BUILD A DATA PIPELINE WITH HIVE

Download a data from the given location - 
https://archive.ics.uci.edu/ml/machine-learning-databases/00360/

1. Create a hive table as per given schema in your dataset----------------------
ANS: 

create table air_quality(
dt string,
time string,
co decimal(10,2),
pt08_s1 decimal(10,2),
nmhc decimal(10,2),
c6h6 decimal(10,2),
pt08_s2 decimal(10,2),
nox decimal(10,2),
pt08_s3 decimal(10,2),
no2 decimal(10,2),
pt08_s4 decimal(10,2),
pt08_s5 decimal(10,2),
t decimal(10,2),
rh decimal(10,2),
ah  decimal(10,2)
)
row format delimited
fields terminated by ','
tblproperties ('skip.header.line.count'='1');



2. try to place a data into table location ----------------------

ANS:  load data local inpath '/home/cloudera/BIGDATA/AirQualityUCI.csv'
into table air_quality


3. Perform a select operation ---------------------------------------------------
ANS: select 
from_unixtime(unix_timestamp(concat(dt,' ',time),'dd-MM-yyyy HH.mm.ss')) 
from air_quality limit 5;



4. Fetch the result of the select operation in your local as a csv file--------------------------

ANS: insert overwrite local directory '/home/cloudera/BIGDATA/newfile'
row format delimited
fields terminated by ','
select * from air_quality;


5. Perform group by operation---------------------------------------------------
select dt, avg(co) AVG_CO, avg(pt08_s1)  AVG_PT08_S1
from air_quality
group by dt;



7. Perform filter operation at least 5 kinds of filter examples . 
Ans:
--Get all data where concentration of CO was less than 1
select * from air_quality where CO<1;

--Get all hours of date='11-03-2004', where concentration of C6H6 was less than 2
select distinct time from air_quality where C6H6 < 2 
and dt='11-03-2004';

--Get all data where concentration of nox was more than 1
select * from air_quality where nox>10;

--Get all data where concentration of nox was more than 1
select * from air_quality where no2>20;

--Get all data where concentration of nox was more than 1
select * from air_quality where PT08.S1>15;


8. show and example of regex operation
ANS:

select * from air_quality where 
 dt rlike '[0-9]{2}-[0-9]{2}-[0-9]{4}'
 

9. alter table operation 
ANS:
 Alter table air_quality add column SO2 decimal(10,2);
 
 
10 . drop table operation
ANS:
drop table air_quality;

12 . order by operation . 
-- ordering by the date
ANS: select * from air_quality
order by from_unixtime(unix_timestamp(concat(dt,' ',time),'dd-MM-yyyy HH.mm.ss'))



13 . where clause operations you have to perform . 
ANS: ALready done in point 7


14 . sorting operation you have to perform .
ANS: 
select * from air_quality
sort by from_unixtime(unix_timestamp(concat(dt,' ',time),'dd-MM-yyyy HH.mm.ss'))

 
15 . distinct operation you have to perform . 
ANS: 
--Get all hours of date='11-03-2004', where concentration of C6H6 was less than 2
select distinct time from air_quality where C6H6 < 2 
and dt='11-03-2004';

16 . like an operation you have to perform . 
ANS: 
select distinct time from air_quality where C6H6 < 2 
and dt like '%11-03%';


17 . union operation you have to perform . 
ANS:

select dt, time, co from air_quality
where dt= '10-03-2004'
union all 
select dt, time, co from air_quality
where dt= '11-03-2004'


18 . table view operation you have to perform . 
ANS: 

create view air_quality_view as
select * from air_quality
where dt in ('10-03-2004','11-03-2004','12-03-2004');

select count(1) from air_quality_view





hive operation with python

Create a python application that connects to the Hive database for extracting data, creating sub tables for data processing, drops temporary tables.fetch rows to python itself into a list of tuples and mimic the join or filter operations



